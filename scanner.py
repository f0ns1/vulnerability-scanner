#!/usr/bin/env python

import requests
import urlparse
import re

class Scanner:
    def __init__(self, url):
        self.target_url=url
        self.session = requests.Session()

    def login(self, data, login):
        self.session.post(login, data=data)
        print("session stored = "+str(self.session))
        return self.session

    def request(self, url, protocol):
        try:
            return self.session.get(protocol + url, allow_redirects=True)
        except Exception as e:
            print("Exception==> "+str(e))
            pass

    def extract_web_links(self,target_url, protocol):
        response = self.request(target_url, protocol)
        print("[+] web target ==> " + protocol + target_url)
        print(response)
        if response:
            return re.findall('(?:href=")(.*?)"', response.content)
        else:
            return []

    def crawling_internal(self, target_url, protocol, list_discover_links, session):
        self.session= session
        href_links = self.extract_web_links(target_url, protocol)
        for link in href_links:
            link = urlparse.urljoin(protocol+target_url,link)
            if "#" in link:
                link = link.split("#")[0]+link.split("#")[1]
            if target_url in link and link not in list_discover_links:
                list_discover_links.append(link)
                #print("\t [+] reference link ==> "+link)
                list_discover_links=self.crawling_internal(link.replace("https://", "").replace("http://",""), protocol, list_discover_links,session)
        return list_discover_links
