#!/usr/bin/env python

import requests
import urlparse
import re

class Scanner:
    def __init__(self, url):
        self.target_url=url
        self.session = requests.Session()
        self.list_discover_links=[]
        self.logout = ["http://10.0.9.6/logout.php"]


    def extract_web_links(self,target_url, protocol):
        if "http://" in target_url:
            protocol=""
        else:
            protocol="http://"
        response  = self.session.get(protocol+target_url)
        if response:
            return re.findall('(?:href=")(.*?)"', response.content)
        else:
            return []

    def crawling_internal(self, protocol, url):
        href_links = self.extract_web_links(url, protocol)
        for link in href_links:
            link = urlparse.urljoin(self.target_url, link)
            if "#" in link:
                link = link.split("#")[0]+link.split("#")[1]
            if protocol+url in link and link not in self.list_discover_links and link not in self.logout:
                print("\t [+] reference link Add to list ==> " + link)
                self.list_discover_links.append(link)
                self.crawling_internal(protocol, link.replace("https://", "").replace("http://",""))
